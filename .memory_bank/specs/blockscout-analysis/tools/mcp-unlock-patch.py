#!/usr/bin/env python3
"""
MCP Unlock Patch for Blockscout API documentation.

Fetches the unlock_blockchain_analysis MCP response, identifies endpoints not yet
documented in the api files generated by api-file-generator.py, and inserts those
missing endpoints into the appropriate api files and master index.

Usage (from repo root):
    python .memory_bank/specs/blockscout-analysis/tools/mcp-unlock-patch.py
"""

from __future__ import annotations

import json
import re
import sys
from pathlib import Path

import requests

# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------

MCP_ENDPOINT = "https://mcp.blockscout.com/v1/unlock_blockchain_analysis"
INDEX_FILE = Path("blockscout-analysis/references/blockscout-api-index.md")
API_DIR = Path("blockscout-analysis/references/blockscout-api")

# ---------------------------------------------------------------------------
# Classification config
# ---------------------------------------------------------------------------

# chain_family → (filename, heading) special-case overrides (spec Section 6.1)
CHAIN_FAMILY_SPECIAL_CASES: dict[str, tuple[str, str]] = {
    "Ethereum Mainnet and Gnosis": ("ethereum.md",      "Ethereum PoS Chains"),
    "zkEVM":                       ("polygon-zkevm.md", "Polygon zkEVM"),
    "zkSync":                      ("zksync.md",        "ZkSync"),
}

# group → (filename, h3_section) for common endpoints (spec Section 6.2)
# Stats group is handled separately (depends on path prefix).
COMMON_GROUP_MAP: dict[str, tuple[str, str]] = {
    "Transactions":    ("transactions.md", "Transactions"),
    "User Operations": ("transactions.md", "User Operations"),
    "Tokens & NFTs":   ("tokens.md",       "Tokens"),
}

# Prefix table for fallback classification of common endpoints (spec Section 6.2).
# MCP paths already include /api prefix, unlike swagger paths in api-file-generator.py.
# Sorted by descending prefix length at module load time.
_DEFAULT_PREFIXES_RAW: list[tuple[str, str]] = [
    ("/api/v2/blocks/",              "blocks.md"),
    ("/api/v2/internal-transactions","transactions.md"),
    ("/api/v2/transactions/",        "transactions.md"),
    ("/api/v2/token-transfers",      "tokens.md"),
    ("/api/v2/addresses/",           "addresses.md"),
    ("/api/v2/tokens/",              "tokens.md"),
    ("/api/v2/smart-contracts/",     "smart-contracts.md"),
    ("/api/v2/search/",              "search.md"),
    ("/api/v1/search",               "search.md"),
    ("/api/v2/stats",                "stats.md"),
    ("/api/v2/main-page/",           "stats.md"),
    ("/api/v2/config/",              "config.md"),
    ("/api/v2/withdrawals",          "ethereum.md"),
    ("/stats-service/",              "stats.md"),
]

_SORTED_PREFIXES: list[tuple[str, str]] = sorted(
    _DEFAULT_PREFIXES_RAW,
    key=lambda x: len(x[0].rstrip("/")),
    reverse=True,
)

# Default H3 section headings per topic file.
TOPIC_H3_HEADINGS: dict[str, str] = {
    "blocks.md":          "Blocks",
    "transactions.md":    "Transactions",
    "addresses.md":       "Addresses",
    "tokens.md":          "Tokens",
    "smart-contracts.md": "Smart Contracts",
    "search.md":          "Search",
    "stats.md":           "Chain Statistics",
    "config.md":          "Configuration",
    "ethereum.md":        "Ethereum PoS Chains",
}

# Fixed topic files in their canonical index order.
TOPIC_FILE_ORDER: list[str] = [
    "blocks.md", "transactions.md", "addresses.md", "tokens.md",
    "smart-contracts.md", "search.md", "stats.md", "config.md",
]

# HTTP method sort order for tie-breaking (spec Section 8.1).
METHOD_ORDER = {"DELETE": 0, "GET": 1, "PATCH": 2, "POST": 3, "PUT": 4}

# ---------------------------------------------------------------------------
# MCP response loading
# ---------------------------------------------------------------------------

def load_mcp_response() -> dict:
    """Fetch the unlock_blockchain_analysis MCP response. Exits on error."""
    print("Loading unlock_blockchain_analysis response (fetched from live endpoint)...")
    try:
        resp = requests.get(MCP_ENDPOINT, timeout=30)
    except requests.RequestException as exc:
        print(f"Error: network error fetching {MCP_ENDPOINT}: {exc}")
        sys.exit(1)

    if resp.status_code != 200:
        print(f"Error: HTTP {resp.status_code} from {MCP_ENDPOINT}")
        sys.exit(1)

    try:
        data = resp.json()
    except ValueError as exc:
        print(f"Error: invalid JSON in MCP response: {exc}")
        sys.exit(1)

    try:
        endpoints = data["data"]["direct_api_endpoints"]
    except (KeyError, TypeError):
        print("Error: 'direct_api_endpoints' key absent from MCP response")
        sys.exit(1)

    # Print summary counts
    common_entries = endpoints.get("common", [])
    specific_entries = endpoints.get("specific", [])
    n_common = sum(len(g.get("endpoints", [])) for g in common_entries)
    n_groups = len(common_entries)
    n_specific = sum(len(c.get("endpoints", [])) for c in specific_entries)
    n_chains = len(specific_entries)
    print(f"  {n_common} common endpoints across {n_groups} groups, "
          f"{n_specific} chain-specific endpoints across {n_chains} chain families")
    print()

    return endpoints

# ---------------------------------------------------------------------------
# Normalised path set
# ---------------------------------------------------------------------------

def _normalise(path: str) -> str:
    """Replace all {param_name} placeholders with {} for parameter-agnostic comparison."""
    return re.sub(r'\{[^}]+\}', '{}', path)


def build_normalised_paths() -> tuple[set[str], int]:
    """
    Read the master index and return (normalised_path_set, total_count).
    Exits with code 1 if the index file is not found.
    """
    try:
        text = INDEX_FILE.read_text(encoding="utf-8")
    except FileNotFoundError:
        print(f"Error: index file not found: {INDEX_FILE}")
        print("Run api-file-generator.py before running this script.")
        sys.exit(1)

    normalised: set[str] = set()
    # Match lines like: - `/path/to/endpoint`: description
    # or:               - /path/to/endpoint: description
    for line in text.splitlines():
        line = line.strip()
        m = re.match(r'^-\s+`?(/[^`:\s]+)`?\s*:', line)
        if m:
            path = m.group(1)
            normalised.add(_normalise(path))

    return normalised, len(normalised)

# ---------------------------------------------------------------------------
# Missing endpoint detection
# ---------------------------------------------------------------------------

def find_missing(
    endpoints: dict,
    normalised_existing: set[str],
) -> list[tuple[dict, str, str]]:
    """
    Return list of (endpoint_dict, group_or_chain_family, source_type) tuples
    for endpoints not yet present in the index.
    source_type is 'common' or 'specific'.
    """
    missing: list[tuple[dict, str, str]] = []

    for group_entry in endpoints.get("common", []):
        group = group_entry.get("group", "")
        for ep in group_entry.get("endpoints", []):
            if _normalise(ep["path"]) not in normalised_existing:
                missing.append((ep, group, "common"))

    for chain_entry in endpoints.get("specific", []):
        chain_family = chain_entry.get("chain_family", "")
        for ep in chain_entry.get("endpoints", []):
            if _normalise(ep["path"]) not in normalised_existing:
                missing.append((ep, chain_family, "specific"))

    return missing

# ---------------------------------------------------------------------------
# Classification
# ---------------------------------------------------------------------------

def _derive_chain_info(chain_family: str) -> tuple[str, str]:
    """Return (filename, h3_heading) for a chain_family value."""
    if chain_family in CHAIN_FAMILY_SPECIAL_CASES:
        return CHAIN_FAMILY_SPECIAL_CASES[chain_family]
    # Auto-derive
    filename = re.sub(r'[\s_]+', '-', chain_family).lower() + ".md"
    heading = re.sub(r'[-_]+', ' ', chain_family).title()
    return filename, heading


def _classify_prefix(path: str) -> tuple[str, str] | None:
    """
    Longest-prefix match on DEFAULT_PREFIXES.
    Returns (filename, h3_section) or None if no match.
    """
    for pfx, fname in _SORTED_PREFIXES:
        pfx_base = pfx.rstrip("/")
        if path == pfx_base or path.startswith(pfx_base + "/"):
            section = TOPIC_H3_HEADINGS.get(fname, fname)
            return fname, section
    return None


def classify_endpoints(
    missing: list[tuple[dict, str, str]],
) -> dict[tuple[str, str], list[dict]]:
    """
    Classify missing endpoints into (filename, h3_section) buckets.
    Returns a dict: (filename, h3_section) → [endpoint_dict, ...]
    """
    result: dict[tuple[str, str], list[dict]] = {}

    for ep, key, source_type in missing:
        path = ep["path"]

        if source_type == "specific":
            filename, heading = _derive_chain_info(key)
            bucket = (filename, heading)

        else:  # common
            group = key
            if group == "Stats":
                if path.startswith("/api/v2/"):
                    bucket = ("stats.md", "Chain Statistics")
                elif path.startswith("/stats-service/"):
                    bucket = ("stats.md", "Stats Service")
                else:
                    result_pfx = _classify_prefix(path)
                    if result_pfx is None:
                        print(f"Warning: cannot classify Stats endpoint (unknown path prefix): {path}")
                        continue
                    bucket = result_pfx
            elif group in COMMON_GROUP_MAP:
                fname, section = COMMON_GROUP_MAP[group]
                bucket = (fname, section)
            else:
                # Fallback: prefix matching
                result_pfx = _classify_prefix(path)
                if result_pfx is None:
                    print(f"Warning: unknown group '{group}' and no path-prefix match for: {path}")
                    continue
                bucket = result_pfx

        result.setdefault(bucket, []).append(ep)

    return result

# ---------------------------------------------------------------------------
# Endpoint entry rendering
# ---------------------------------------------------------------------------

def _infer_type(param_name: str) -> str:
    """Infer the type of a path parameter from its name (spec Section 7.4)."""
    if param_name.endswith("_number") or param_name.endswith("_count"):
        return "integer"
    if param_name in ("epoch_number", "batch_number", "instance_id"):
        return "integer"
    return "string"


def render_endpoint(ep: dict) -> str:
    """
    Render a single endpoint entry in the api-format-spec.md format.
    Returns the entry as a string (with trailing newline).
    """
    path = ep["path"]
    description = ep.get("description", "")

    lines: list[str] = []
    lines.append(f"#### GET {path}")
    lines.append("")
    if description:
        lines.append(description)
        lines.append("")

    # Extract path parameters
    param_names = re.findall(r'\{([^}]+)\}', path)

    lines.append("- **Parameters**")
    lines.append("")
    if param_names:
        lines.append("  | Name | Type | Required | Description |")
        lines.append("  | ---- | ---- | -------- | ----------- |")
        for name in param_names:
            ptype = _infer_type(name)
            lines.append(f"  | `{name}` | `{ptype}` | Yes |  |")
    else:
        lines.append("  *None*")

    lines.append("")
    return "\n".join(lines)

# ---------------------------------------------------------------------------
# API file patching
# ---------------------------------------------------------------------------

def _sort_key(path_method: tuple[str, str]) -> tuple[str, int]:
    path, method = path_method
    return path.lower(), METHOD_ORDER.get(method, 99)


def _extract_entries_in_section(
    lines: list[str], section_start: int, section_end: int
) -> list[tuple[str, str, list[str]]]:
    """
    Within lines[section_start:section_end], collect existing #### entries.
    Returns list of (path, method, entry_lines) where entry_lines includes
    the #### heading line through the last content line before the next entry.
    """
    entries: list[tuple[str, str, list[str]]] = []
    i = section_start
    while i < section_end:
        line = lines[i]
        m = re.match(r'^#### (GET|POST|PUT|PATCH|DELETE) (/\S+)', line)
        if m:
            method = m.group(1)
            path = m.group(2)
            entry_lines = [line]
            j = i + 1
            while j < section_end:
                if re.match(r'^####', lines[j]):
                    break
                entry_lines.append(lines[j])
                j += 1
            entries.append((path, method, entry_lines))
            i = j
        else:
            i += 1
    return entries


def _render_entries_block(entries: list[tuple[str, str, list[str]]]) -> str:
    """Render a sorted list of (path, method, entry_lines) into a string."""
    sorted_entries = sorted(entries, key=lambda e: _sort_key((e[0], e[1])))
    parts: list[str] = []
    for _, _, entry_lines in sorted_entries:
        # Strip trailing blank lines from each entry, then add exactly one blank line separator
        text = "\n".join(entry_lines).rstrip()
        parts.append(text)
    return "\n\n".join(parts) + "\n"


def patch_api_file(
    filename: str,
    h3_section: str,
    new_endpoints: list[dict],
) -> str:
    """
    Patch an API file by inserting new_endpoints into the h3_section.
    Returns a status string for console output.
    """
    filepath = API_DIR / filename
    new_rendered = [(ep["path"], "GET", render_endpoint(ep).rstrip().splitlines()) for ep in new_endpoints]

    if not filepath.exists():
        # Create new file (spec Section 8.3)
        entries_block = _render_entries_block(new_rendered)
        content = f"## API Endpoints\n\n### {h3_section}\n\n{entries_block}"
        try:
            filepath.write_text(content, encoding="utf-8")
        except OSError as exc:
            print(f"Error: cannot write {filepath}: {exc}")
            sys.exit(1)
        count = len(new_endpoints)
        return f"Created: blockscout-api/{filename} ({count} endpoint{'s' if count != 1 else ''})"

    # File exists — read it
    text = filepath.read_text(encoding="utf-8")
    lines = text.splitlines()

    # Find the target H3 section
    section_line_idx: int | None = None
    for i, line in enumerate(lines):
        if line.strip() == f"### {h3_section}":
            section_line_idx = i
            break

    if section_line_idx is None:
        # Section not found — append it (spec Section 8.2)
        entries_block = _render_entries_block(new_rendered)
        new_section = f"\n### {h3_section}\n\n{entries_block}"
        content = text.rstrip() + "\n" + new_section
        try:
            filepath.write_text(content, encoding="utf-8")
        except OSError as exc:
            print(f"Error: cannot write {filepath}: {exc}")
            sys.exit(1)
        count = len(new_endpoints)
        return (f"Patched: blockscout-api/{filename} "
                f"(added {count} endpoint{'s' if count != 1 else ''} to ### {h3_section})")

    # Section found — determine its extent (ends at next ## heading or EOF)
    section_end = len(lines)
    for i in range(section_line_idx + 1, len(lines)):
        if re.match(r'^##[^#]', lines[i]):
            section_end = i
            break

    # Extract existing entries within the section
    existing = _extract_entries_in_section(lines, section_line_idx + 1, section_end)
    all_entries = existing + new_rendered

    # Reconstruct file: everything before section body + sorted entries + remainder
    prefix_lines = lines[:section_line_idx + 1]  # up to and including the ### line
    suffix_lines = lines[section_end:]

    entries_block = _render_entries_block(all_entries)
    prefix_text = "\n".join(prefix_lines)
    suffix_text = "\n".join(suffix_lines)

    # Build final content
    if suffix_text.strip():
        content = prefix_text + "\n\n" + entries_block + suffix_text
    else:
        content = prefix_text + "\n\n" + entries_block

    # Ensure single trailing newline
    content = content.rstrip() + "\n"

    try:
        filepath.write_text(content, encoding="utf-8")
    except OSError as exc:
        print(f"Error: cannot write {filepath}: {exc}")
        sys.exit(1)

    count = len(new_endpoints)
    return (f"Patched: blockscout-api/{filename} "
            f"(added {count} endpoint{'s' if count != 1 else ''} to ### {h3_section})")

# ---------------------------------------------------------------------------
# Index file patching
# ---------------------------------------------------------------------------

def _make_index_line(ep: dict) -> str:
    """Format a single index line item."""
    path = ep["path"]
    desc = ep.get("description", "")
    return f"- `{path}`: {desc}"


def _get_display_name_for_file(filename: str, h3_section: str) -> str:
    """Return the display name (index H2 heading text) for a given api file."""
    # For topic files, use their canonical display names
    topic_display = {
        "blocks.md":          "Blocks",
        "transactions.md":    "Transactions",
        "addresses.md":       "Addresses",
        "tokens.md":          "Tokens",
        "smart-contracts.md": "Smart Contracts",
        "search.md":          "Search",
        "stats.md":           "Stats",
        "config.md":          "Configuration",
    }
    if filename in topic_display:
        return topic_display[filename]
    # For chain files, use the H3 heading
    return h3_section


def patch_index_file(
    file_sections: dict[str, list[tuple[str, dict]]],
    existing_count: int,
) -> int:
    """
    Update the master index file with new endpoint entries.
    file_sections: {filename: [(h3_section, endpoint_dict), ...]}
    Returns new total path count.
    """
    text = INDEX_FILE.read_text(encoding="utf-8")
    lines = text.splitlines()

    # Process each file that has new endpoints
    for filename, section_eps in file_sections.items():
        # Collect new line items grouped by section
        # (for the index, all endpoints from the same file go into the same section)
        new_lines = [_make_index_line(ep) for _, ep in section_eps]

        # Find the H2 section for this file in the index
        file_ref = f"blockscout-api/{filename}"
        section_idx: int | None = None
        for i, line in enumerate(lines):
            # Match: ## [DisplayName](blockscout-api/filename.md) or similar
            if re.search(re.escape(file_ref), line) and line.startswith("## "):
                section_idx = i
                break

        if section_idx is not None:
            # Find the extent of this section (until next ## or EOF)
            section_end = len(lines)
            for i in range(section_idx + 1, len(lines)):
                if lines[i].startswith("## "):
                    section_end = i
                    break

            # Collect existing list items
            existing_items: list[str] = []
            non_item_lines: list[str] = []  # preamble lines (non-list, non-blank after heading)
            for i in range(section_idx + 1, section_end):
                line = lines[i]
                if re.match(r'^-\s+', line):
                    existing_items.append(line)
                else:
                    non_item_lines.append((i, line))

            # Merge and sort all items
            all_items = existing_items + new_lines
            all_items.sort(key=lambda l: _normalise(re.match(r'^-\s+`?(/[^`:\s]+)', l).group(1)).lower()
                           if re.match(r'^-\s+`?(/[^`:\s]+)', l) else l.lower())

            # Reconstruct the section
            heading_line = lines[section_idx]
            new_section_lines = [heading_line, ""]
            # Re-add non-list preamble lines (e.g., ethereum.md description)
            preamble = [l for _, l in non_item_lines if l.strip() and not l.startswith("-")]
            if preamble:
                new_section_lines.extend(preamble)
                new_section_lines.append("")
            new_section_lines.extend(all_items)

            # Replace the section in lines
            lines = lines[:section_idx] + new_section_lines + lines[section_end:]

        else:
            # New chain section — insert in alphabetical position among chain sections
            # Find where chain sections start (after last topic section)
            topic_refs = {f"blockscout-api/{f}" for f in TOPIC_FILE_ORDER}
            last_topic_end = 0
            for i, line in enumerate(lines):
                if line.startswith("## "):
                    ref_m = re.search(r'\(blockscout-api/([^)]+)\)', line)
                    if ref_m and f"blockscout-api/{ref_m.group(1)}" in topic_refs:
                        # Find end of this section
                        end = len(lines)
                        for j in range(i + 1, len(lines)):
                            if lines[j].startswith("## "):
                                end = j
                                break
                        last_topic_end = end

            # Collect chain section headings and their positions
            chain_sections: list[tuple[str, int]] = []  # (filename, start_line_idx)
            i = last_topic_end
            while i < len(lines):
                line = lines[i]
                if line.startswith("## "):
                    ref_m = re.search(r'\(blockscout-api/([^)]+)\)', line)
                    if ref_m:
                        chain_sections.append((ref_m.group(1), i))
                i += 1

            # Determine insertion position alphabetically by filename
            insert_before_idx = len(lines)  # default: append at end
            for chain_fname, chain_line_idx in chain_sections:
                if filename.lower() < chain_fname.lower():
                    insert_before_idx = chain_line_idx
                    break

            # Determine display name (h3_section of first entry for this file)
            display_name = _get_display_name_for_file(
                filename,
                section_eps[0][0] if section_eps else filename
            )
            new_section_text = [
                "",
                f"## [{display_name}](blockscout-api/{filename})",
                "",
            ]
            new_section_text.extend(new_lines)

            lines = lines[:insert_before_idx] + new_section_text + lines[insert_before_idx:]

    # Write updated index
    content = "\n".join(lines)
    if not content.endswith("\n"):
        content += "\n"
    INDEX_FILE.write_text(content, encoding="utf-8")

    # Recount paths in updated index
    _, new_count = build_normalised_paths()
    return new_count

# ---------------------------------------------------------------------------
# Console output helpers
# ---------------------------------------------------------------------------

def _format_classification_summary(
    classified: dict[tuple[str, str], list[dict]],
    new_files: set[str],
) -> None:
    """Print aligned classification lines."""
    if not classified:
        return

    # Build rows: (filename, h3_section, count, is_new)
    rows = []
    for (fname, section), eps in classified.items():
        rows.append((fname, section, len(eps), fname in new_files))

    # Sort: topic files first (in order), then chain files alphabetically
    topic_set = set(TOPIC_FILE_ORDER)
    topic_order = {f: i for i, f in enumerate(TOPIC_FILE_ORDER)}
    rows.sort(key=lambda r: (0 if r[0] in topic_set else 1,
                              topic_order.get(r[0], 999),
                              r[0].lower()))

    max_fname_len = max(len(r[0]) for r in rows)
    max_sec_len = max(len(f"### {r[1]}") for r in rows)

    for fname, section, count, is_new in rows:
        ep_word = "endpoint" if count == 1 else "endpoints"
        new_tag = "  [NEW FILE]" if is_new else ""
        print(f"  {fname:<{max_fname_len}}  ({f'### {section}':<{max_sec_len}}):  "
              f"{count} {ep_word}{new_tag}")

# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main() -> None:
    # 1. Load MCP response
    endpoints = load_mcp_response()

    # 2. Build normalised path set from existing index
    normalised_existing, existing_count = build_normalised_paths()
    print(f"Reading existing index: {existing_count} documented paths")
    print()

    # 3. Identify missing endpoints
    missing = find_missing(endpoints, normalised_existing)
    print(f"Identifying missing endpoints: {len(missing)}")
    print()

    if not missing:
        print("Nothing to patch.")
        print()
        print("Done.")
        return

    # 4. Classify missing endpoints
    classified = classify_endpoints(missing)

    # Determine which files are new (don't exist yet)
    new_files: set[str] = set()
    for (fname, _) in classified:
        if not (API_DIR / fname).exists():
            new_files.add(fname)

    print("Classifying...")
    _format_classification_summary(classified, new_files)
    print()

    # 5. Patch API files
    print("Patching API files...")
    for (fname, section), eps in sorted(
        classified.items(),
        key=lambda item: (0 if item[0][0] in set(TOPIC_FILE_ORDER) else 1, item[0][0].lower())
    ):
        status = patch_api_file(fname, section, eps)
        print(f"  {status}")
    print()

    # 6. Update index file
    # Build file_sections: {filename: [(h3_section, ep), ...]}
    file_sections: dict[str, list[tuple[str, dict]]] = {}
    for (fname, section), eps in classified.items():
        for ep in eps:
            file_sections.setdefault(fname, []).append((section, ep))

    new_count = patch_index_file(file_sections, existing_count)
    print(f"Updating blockscout-api-index.md: {existing_count} → {new_count} endpoints")
    print()

    print("Done.")


if __name__ == "__main__":
    main()
